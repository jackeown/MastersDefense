<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Jack McKeown Masters Defense</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/blood.css">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/monokai.css">
</head>

<body>
	<style>
		.small {
			transform: scale(0.7);
			font-size: 0.7em;
		}

		.red {
			color: rgb(255, 150, 150);
		}

		.green {
			color: rgb(150, 255, 150);
		}

		.yellow {
			color: rgb(255, 255, 123);
		}

		.row {
			display: flex;
			flex-direction: row;
			justify-content: space-around;
		}

		.box {
			/* background:rgba(200,200,200,0.1);*/
			box-shadow:2px 2px 15px -3px rgba(50,50,50,1.0);
			border: 2px solid rgb(80,80,80);
			padding: 15px;
			margin: 5px;
			font-size: 24pt;
		}

		.emph {
			font-style: italic;
		}

		.ul {
			text-decoration: underline;
		}

		.reveal ul,
		.reveal ul li ul {
			list-style: none;
			/* Remove default bullets */
		}

		ul>li::before {
			content: "";
			/* Add content: \2022 is the CSS Code/unicode for a bullet */
			background: red;
			/* Change the color */
			font-weight: bold;
			/* If you want it to be bold */
			display: inline-block;
			/* Needed to add space between the bullet and the text */
			border-radius: 50%;

			margin-left: -24px;
			margin-right: 8px;
			margin-bottom: 4px;
			width: 10px;
			height: 10px;
		}

		ul li ul li::before {
			content: "";
			/* Add content: \2022 is the CSS Code/unicode for a bullet */
			background: none;
			border: 2px solid red;
			/* Change the color */
			font-weight: bold;
			/* If you want it to be bold */
			display: inline-block;
			/* Needed to add space between the bullet and the text */
			border-radius: 50%;

			margin-left: -24px;
			margin-right: 8px;
			margin-bottom: 4px;
			width: 8px;
			height: 8px;
		}

		.twoOverOneGrid {
			display: grid;
			grid-template-areas:
				"topLeft topRight"
				"bottom bottom";
		}

		.twoOverOneGrid>*:nth-child(1) {
			grid-area: topLeft;
		}

		.twoOverOneGrid>*:nth-child(2) {
			grid-area: topRight;
		}

		.twoOverOneGrid>*:nth-child(3) {
			grid-area: bottom;
		}

		.twoOverTwoGrid {
			display: grid;
			grid-template-areas:
				"topLeft topRight"
				"bottomLeft bottomRight";
		}

		.twoOverOneGrid>*,
		.twoOverTwoGrid>* {
			font-size: 0.5em;
		}

		.vertical {
			display: flex;
			flex-direction: column;
			justify-content: space-between;
		}

		img {
			background: white;
			max-width:100%;
		}

		.reveal h3{
			font-size:36pt;
		}

		.figure{
			width:650px;
		}

		.smallimg{
			width:21rem;
		}

	</style>
	<div class="reveal">
		<div class="slides">
			<section>
				<h3>Clause Representation for Proof Guidance using Neural Networks</h3>
				<span>John (Jack) McKeown</span><br>
				<span class="small">Supervised by Dr. Geoff Sutcliffe</span>
			</section>

			<section>
				<h3>Outline</h3>
				<div class="box" style="width:70%;margin:auto;">
					<ol style="color:red;">
						<li><span style="color:whitesmoke;">Introduction to Automated Reasoning</li>
						<li><span style="color:whitesmoke;">Given Clause Selection</span></li>
						<li><span style="color:whitesmoke;">Research Goals</span></li>
						<li><span style="color:whitesmoke;">MPTPTP2078 Dataset</span></li>
						<li><span style="color:whitesmoke;">The E Theorem Prover</span></li>
						<li><span style="color:whitesmoke;">Previous Work</span></li>
						<li><span style="color:whitesmoke;">Supervised Approaches</span></li>
						<li><span style="color:whitesmoke;">Unsupervised Approaches</span></li>
						<li><span style="color:whitesmoke;">Results</span></li>
						<li><span style="color:whitesmoke;">Future Research</span></li>
					</ol>
				</div>
			</section>

			<section id="ATP">
				<h3>Automated Reasoning</h3>

				<section>
					<div class="twoOverTwoGrid">
						<div class="box vertical fragment">
							<h4>Propositional Logic</h4><br>
							<ul>
								<li>Boolean satisfiability is NP-Complete</li>
								<li>Surprisingly fast heuristic solvers exist nonetheless</li>
							</ul>
							<div style="border-top: 1px solid white; text-align:left; margin-top:1em;">
								<span class="ul">Example:</span><br>
								<span class="green">
									$ jack\_is\_happy \implies jack\_drank\_coffee$
								</span>
							</div>
						</div>

						<div class="box vertical fragment">
							<h4>First Order Logic</h4><br>
							<ul>
								<li>Complete (all tautologies are theorems)</li>
								<li>Practical for more real world problems</li>
							</ul>
							<div style="border-top: 1px solid white; text-align:left; margin-top:1em;">
								<span class="ul">Example:</span><br>
								<span class="green">
									$ \forall X (happy(X) \implies drank(X, coffee)) $
								</span>
							</div>
						</div>

						<div class="box vertical fragment">
							<h4>Typed First Order Logic</h4><br>
							<ul>
								<li>Types allow for static type-checking to make problem formulation
									easier.</li>
							</ul>
							<div style="border-top: 1px solid white; text-align:left; margin-top:1em;">
								<span class="ul">Example:</span><br>
								<span class="green">
									$ \forall (X:person) (happy(X) \implies drank(X, coffee)) $
								</span>
							</div>
						</div>

						<div class="box vertical fragment">
							<h4>Higher Order Logic</h4><br>
							<ul>
								<li>Incomplete</li>
								<li>Quantification over functions/predicates</li>
								<li>Very expressive but very abstract</li>
								<li>Types necessary to avoid Russell's Paradox</li>
							</ul>
							<!-- <div class="fragment" style="border-top: 1px solid white; text-align:left; margin-top:1em;">
									<span class="ul">Example:</span><br>
									<span class="green">
										¯\_(ツ)_/¯
									</span>
								</div> -->
						</div>
					</div>

					<aside class="notes">
						Automated Reasoning is the study of logic from a computational perspective.
						The simplest form of logic is propositional logic.
					</aside>
				</section>

				<section>
					<div class="box">
						<h4>Automated Theorem Proving</h4><br>
						<ul>
							<li class="fragment">Proving conjectures to be theorems of a set of axioms using
								computational methods</li>
							<li class="fragment"><span class="emph green">Saturation</span> means finding the closure of
								set of formulae under a set of inference rules.</li>
							<li class="fragment">Proofs by contradiction via saturating $axioms \cup \{\neg
								conjecture\}$</li>
						</ul>
					</div>
				</section>

			</section>

			<section id="GCS">
				<h3>Given Clause Selection</h3>

				<section>
					<div class="box">
						<h4>Converting Formulae to Clauses</h4>
						<span class="red">$$\forall X \exists Y (p(Y) \; | \; (q \; \& \; r(X)))$$</span>
						becomes<br><br>
						$\{\; $<span class="green">$p(sk(X))\;|\;q$</span>$,\;$ <span class="green">$p(sk(X))\;|\;r(X)$</span>$ \;\} $
					</div>
				</section>

				<section>
					<img src="images/processedAndUnprocessedSets.svg">
					<ul>
						<li style="font-size: 0.7em;">Improving given clause selection is a core goal of ATP research</li>
					</ul>
				</section>

				<aside class="notes">
					Saturation based theorem provers typically convert first order formulae into clauses.
					Clauses have no "and" internally and have only universally quantified variables.
				</aside>
			</section>

			<section id="researchGoals">
				<h3>Research Goals</h3>
				<section>
					<h4>Clause Embedding</h4>
					<div style="display:flex; width:100%">
						<div class="box"
							style="font-family:monospace; font-size:0.4em;display: grid; justify-content:center;align-items:center;">
							<div>
								p<span class="green">(</span>f<span class="red">(</span>X,a<span class="red">)</span>,
								a<span class="green">)</span>
								|
								q<span class="green">(</span>f<span class="red">(</span>X,a<span class="red">)</span>,
								g<span class="red">(</span>a,b,c<span class="red">)</span><span class="green">)</span>
								| c!=s
							</div>
						</div>
						<div style="margin:auto;">
							$\rightarrow$
						</div>
						<div class="box" style="width:20%;display:grid;justify-content:center;align-items:center;">
							<img src="images/clauseDag.png">
						</div>
						<div style="margin:auto;">
							$\rightarrow$
						</div>
						<div class="box" style="display:grid;justify-content:center;align-items:center;">
							$$
							\newcommand\mycolv[1]{\begin{bmatrix}#1\end{bmatrix}}
							\mycolv{1.2\\-0.2\\0.9\\ \vdots}
							$$
						</div>
					</div>
				</section>

				<section>
					<div style="font-family: monospace; font-size:0.8em;">
						p<span class="green">(</span>f<span class="red">(</span>X,a<span class="red">)</span>,
						a<span class="green">)</span>
						|
						q<span class="green">(</span>f<span class="red">(</span>X,a<span class="red">)</span>,
						g<span class="red">(</span>a,b,c<span class="red">)</span><span class="green">)</span>
						| c!=s
					</div>
					<img src="images/clauseDag.png">
				</section>

				<section>
					<h4>Classification</h4>
					<div style="display:flex">
						<div class="box" style="font-size:0.5em;">
							$$
							\underbrace{
							\mycolv{1.2\\-0.2\\0.9\\ \vdots}
							}_\text{Given Clause},
							\underbrace{
							\begin{bmatrix}
							0.3 & -1.1 & 0.7 & \cdots \\
							1.3 & -0.3 & -0.3& \cdots \\
							0.1 & 0.24 & 0.2& \cdots \\
							\vdots & \vdots & \vdots & \ddots
							\end{bmatrix}
							}_\text{Context Clauses}
							$$
						</div>
						<div style="margin:auto;">
							$\rightarrow$
						</div>
						<div class="box" style="margin:auto">
							Classifier
						</div>
						<div style="margin:auto;">
							$\rightarrow$
						</div>
						<div class="box" style="width:100%;display:grid;justify-content: center;align-items:center;">
							<span>
								<span class="green">Select</span> or <span class="red">Don't</span><br>
								<span style="font-size:0.5em;">
									Within E, the output becomes the priority for a priority queue.
								</span>
							</span>
						</div>
					</div>
				</section>

				<section>
					<h4>Putting It All Together</h4>
					<div class="box">
						<img src="images/architecture.svg" alt="architecture" width=550>
					</div>
				</section>
			</section>

			<section id="MPTPTP2078">
				<h3>MPTPTP2078 Dataset</h3>
				<div class="box" style="font-size:0.8em;">
					<ul>
						<li class="fragment">Mizar Mathematical Library</li>
						<li class="fragment"><a href="https://github.com/JUrban/MPTP2078">MPTP2078</a> - <span
								class="emph green">bushy</span> and <span class="emph red">chainy</span> variants of 2078
							problems leading up to a specific theorem in MML</li>
						<li class="fragment"><a href="https://github.com/TPTPWorld/MPTPTP2078">MPTPTP2078</a> - A refined
							version of MPTP2078</li>
						
						<li class="fragment">(Bushy variants are used in my research)</li>
					</ul>
				</div>
			</section>

			<section>
				<h3>The E Theorem Prover</h3>
				<div class="box">
					<ul>
						<li>
							<a href="https://eprover.org">E</a> is a state of the art first
							order saturation based theorem prover written in C by 
							<a href="https://wwwlehre.dhbw-stuttgart.de/~sschulz/DHBW_Stephan_Schulz/Stephan_Schulz.html">Dr. Stephan Schulz</a>
						</li>
						<li>E supports custom clause evaluation functions.</li>
					</ul>
				</div>
			</section>

			<section id="previousWork">
				<h3 data-id="title">Previous Work and Background</h3>
				<section>
					<div class="box" style="font-size:0.6em;">
						<h4>Graph Neural Networks (GNNs)</h4>
						<ul>
							<li>
								The term <span class="green">Graph Neural Network</span> was coined in 2005 by Gori et al.
							</li>
							<li>Nodes have feature vectors.</li>
							<li>Nodes get updated as a function of the sum of their neighbors' feature vectors.</li>
						</ul>
						<img class="smallimg" src="images/propagation.svg">
						$$ x_i = \displaystyle f(x_i, \sum_{x_j \in N(x_i)} x_j) $$

						<!-- <blockquote></li>
							More precisely, let $w$ be a
							set of parameters and $f_w$ be a parametric transition function
							that expresses the dependence of a node on its neighborhood.
							The state [of graph node $n$], $x_n$, is defined as the solution of the system of equations:
	
							$$x_n = f_w(l_n, x_{ne[n]},l_{ne[n]}), n \in \mathbb{N}$$
	
							where $l_n$, $x_{ne[n]}$, $l_{ne[n]}$ are the label of $n$, and the states and
							the labels of the nodes in the neighborhood of $n$, respectively.
						</blockquote>
						In their implementation:<br><br>
						<ul>
							<li>$f_w$ is a fully connected neural net</li>
							<li>$x_{ne[n]}$ are summed.</li>
							<li>$l_{ne[n]}$ are summed.</li>
						</ul> -->
					</div>
				</section>

				<section>
					<div class="box" style="font-size:0.6em;">
						<h4>Graph Convolutional Networks (GCNs)</h4>
						<ul>
							<li>Defined in 2017 by Kipf and Welling</li>
							<li>Fixes problem where nodes with high degree drown out other nodes</li>
							<!-- <li>$H^{(l+1)}= \sigma\!\left(\tilde{D}^{-\frac{1}{2}} \tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)} W^{(l)} \right) \, $</li>
							<li>$\tilde{D}^{-\frac{1}{2}} \tilde{A}\tilde{D}^{-\frac{1}{2}}$ does not require explicit access to the adjacency matrix.</li>
							<li>Has the effect of <span class="emph">quieting</span> the nodes with large degree.</li> -->
						</ul>
					</div>
				</section>

				<section>
					<div class="box" style="font-size:0.5em;">
						<h4>Message Passing Neural Networks (MPNNs)</h4>

						<ul>
							<li>Introduced by Gilmer et al. in 2017</li>
							<li>Messages are calculated for each edge: $M(a,e,b)$.</li>
							<li>Messages coming into a given node are summed and used to update node state as in normal GNNs.</li>
						</ul><br>
						<img src="images/edgeMessage.svg" class="smallimg">
						
					</div>
				</section>

				<section>
					<div class="box" style="font-size:0.7em;">
						<h4>Other Models</h4>
						<ul>
							<li>TreeLSTM / DagLSTM</li>
							<li>Recursive Neural Networks (not recurrent)</li>
						</ul>
					</div>
				</section>

				<section>
					<div class="box" style="font-size:0.7em;">
						<h4>Autoencoder Background</h4>
						<ul>
							<li>Train $\theta=($<span class="red">$\theta_D$</span>,<span
									class="green">$\theta_E$</span>$)$ so that <span
									class="red">$Decode_{\theta_D}$</span>$($<span
									class="green">$Encode_{\theta_E}$</span>$(x)) \approx x$</li>
							<li>Typically used for nonlinear dimensionality reduction</li>
							<li>
								The architecture of <span class="green">$Encode$</span> and <span
									class="red">$Decode$</span> is chosen so that <br>
								<span class="green">$Encode$</span>$(x)$ has some desired property:
								<ul>
									<li class="fragment"><span class="green">$Encode$</span>$(x)$ typically has lower
										dimensionality than $x$</li>
									<li class="fragment"><span class="green">$Encode$</span>$(x)$ could be forced to be
										sparse</li>
									<li class="fragment"><span class="green">$Encode$</span>$(x)$ could be a
										distribution which is sampled from before decoding...</li>
								</ul>
							</li>
						</ul>
					</div>
				</section>

				<section>
					<div class="box" style="font-size:0.7em;">
						<h4>Machine Learning for ATP</h4>
						<ul>
							<li><span class="green">Google:</span> HolStep dataset and GNNs for higher order logic</li>
							<li><span class="green">Enigma:</span> Hand crafted features classified with XGBoost</li>
							<li><span class="green">Enigma-NG:</span> Recursive Neural Network</li>
						</ul>
					</div>
				</section>
			</section>

			<section>
				<h3>Supervised Approaches</h3>
				<section>
					<div class="box" style="font-size:0.65em;">
						<h4 style="color:rgb(150, 255, 150);" class="emph">My Custom Model</h4>
						<ul>
							<li><span class="emph green">Asynchronous message passing</span> (uses topological sorting)</li>
							<li><span class="emph green">Attention</span> instead of simple sum</li>
							<li><span class="emph green">Readout via root</span> rather than via global pooling</li>
							<li><span class="red">Hard to parallelize (super slow)</span></li>
						</ul>
						<div class="r-stack">
							<img class="smallimg " src="images/async/Async Dag Processing1.svg">
							<img class="smallimg fragment fade-in-then-out" src="images/async/Async Dag Processing2.svg">
							<img class="smallimg fragment fade-in-then-out" src="images/async/Async Dag Processing3.svg">
							<img class="smallimg fragment fade-in-then-out" src="images/async/Async Dag Processing4.svg">
						</div>
					</div>
				</section>

				<section>
					<div class="box">
						<h4 style="color:rgb(150, 255, 150);" class="emph">My 2-Layer PyTorch Geometric (PyG) GCN</h4>
						<ul>
							<li> <span class="green">Simple</span></li>
							<li> <span class="green">Parallelism handled by PyG</span></li>
						</ul>
						<div class="r-stack">
							<img class="smallimg " src="images/async/Sync Dag Processing1.svg">
							<img class="smallimg fragment fade-in-then-out" src="images/async/Sync Dag Processing2.svg">
							<img class="smallimg fragment fade-in-then-out" src="images/async/Sync Dag Processing3.svg">
							<img class="smallimg fragment fade-in-then-out" src="images/async/Sync Dag Processing4.svg">
						</div>
					</div>
				</section>

				<section>
					<div class="box">
						<h4>Classifiers</h4>
						<ul>
							<li>Both clause embedding models are trained with the same classifier architecture</li>
							<li>The classifier sums the context vectors and then concatenates this sum with the given clause vector</li>
							<li>This concatenation then goes through a simple MLP with ReLU activation</li>
						</ul>
					</div>
				</section>
			</section>

			<section>
				<h3>Unsupervised Approaches</h3>

				<section>
					<div class="box" style="height:15em;">
						<h4>Formulae Autoencoder</h4>
						<a href="https://arxiv.org/pdf/2101.09142.pdf">Purgal et al.</a>
						<div class="r-stack" style="width:20em; height:10em;margin:auto;">
							<img src="images/extractors.png" alt="extractors" class="fragment fade-in-then-out"
								style="margin:auto;">
							<img src="images/differenceTraining.png" alt="differenceTraining"
								class="fragment fade-in-then-out" style="margin:auto;">
						</div>
					</div>
				</section>

				<section>
					<div class="box">
						<h4>GNN $\geq$ LSTM/Transformers/etc</h4>
						<div class="green emph">(My Clause Autoencoder)</div>
						<div style="display:flex; flex-flow: row wrap; justify-content: space-around;">
								<img src="images/ClauseAE1.svg" alt="ClauseAE1.svg" width="350">
								<img src="images/ClauseAE2.svg" alt="ClauseAE2.svg" width="350">
						</div>
					</div>
				</section>

				<section>
					<div class="box" style="width:18em;margin:auto;">
						<h4>Recursive Training</h4>
						<a href="https://arxiv.org/pdf/2101.09142.pdf">Purgal et al.</a>
						<img src="images/recursiveTraining.png" alt="recursiveTraining"
							style="margin:auto;margin-top:10px;">
					</div>
				</section>

				<section>
					<div class="box">
						<h5 class="emph" style="color:rgb(150, 255, 150);">My Variational Clause Autoencoder</h5>
						<img src="images/ClauseVAE.svg" alt="ClauseVAE.svg" width="600">
					</div>
				</section>
			</section>

			<section>
				<h3>Model Evaluation</h3>
				<section>
					<div class="box">
						<h4>Models to Evaluate</h4>
						<ul>
							<li>My Custom Model</li>
							<li>My Simple 2-layer PyG GCN model</li>
							<li>My Ordered DAG Autoencoder</li>
							<li>My Variational Ordered DAG Autoencoder</li>
						</ul>
					</div>
				</section>

				<section>
					<div class="box">
						<h4>Explicit Clause Objectives</h4>
						<ul style="font-size:0.8em;">
							<li><span class="emph green">Horn</span> - Is a clause a Horn clause (at most one positive
								literal)?</li>
							<li><span class="emph green">NumNodes</span> - How many nodes does a clause graph contain?
							</li>
							<li><span class="emph green">NumDistinctVars</span> - How many distinct variables occur in
								the clause?</li>
							<li><span class="emph green">NumEqualities</span> - How many “=” symbols occur in the
								clause?</li>
							<li><span class="emph green">NumNegations</span> - How many negative literals are in the
								clause?</li>
							<li><span class="emph green">MaxDepth</span> - What is the maximum nesting of subterms in
								the clause?</li>
						</ul>
					</div>
				</section>


				<section>
					<h4>Overview</h4>
					<div class="box small">
						<!-- Which models work well for which objectives? -->
						<!-- AE/VAE vs GCN vs SimpleEmbedding -->
						<!-- <table>
							<tr><th></th><th></th><th></th></tr>
							<tr><th>c</th><td>hello</td><td>hello</td></tr>
							<tr><th>d</th><td>hello</td><td>hello</td></tr>
						</table>
						<br> -->
						<ul>
							<li>Autoencoder models summarize DAGs by their root, while the GCN model summarizes DAGs by global pooling</li>
							<li><span class="green emph">MaxDepth</span> Objective benefits greatly from root summary</li>
							<li>
								A simple vector of symbol counts leads to the highest (or tied for highest)
								accuracy in the <span class="green emph">NumNodes</span> and <span class="green emph">NumEqualities</span> objectives.
							</li>
							<li>
								The <span class="green emph">NumDistinctVars</span> objective accuracy was maximized by the GCN model
								and the autoencoding models yielded lower accuracy. (98% vs 99.5%)
							</li>
						</ul>
					</div>
				</section>


				<section>
					<div class="box" style="font-size:0.5em;">
						<h4><span class="green emph">Horn</span> Explicit Clause Objectives</h4>
						<img class="figure" src="images/Horn.png"><br>
						
						<aside class="notes">
							The four best performing models overlap and are all tied for 100% accuracy with the next four models all obtaining
							accuracy above 99.5%. Overall, there are eight models that finish training with over 95% accuracy.
							The three visible but distinctly poorly-performing embedding models (still 96%-97%) were all randomly initialized and were not allowed to update during training.
						</aside>
					</div>
				</section>

			
				
				<section>
					<div class="box" style="font-size:0.5em;">
						<h4><span class="green emph">NumNodes</span> Explicit Clause Objectives</h4>
						<img class="figure" src="images/NumNodes.png"><br>
						
						<aside class="notes">
							The <span class="emph green">SimpleEmbedding</span> is roughly tied with the both the pretrained and "\_base" refined <span class="emph green">DAGEmbeddingGeom</span> models at ~93%.
							The main insight from this plot (and many of these other plots) is that while a simple symbol counting embedding can encode the total number of nodes in a DAG,
							embeddings (like the autoencoder embeddings) that obtain their final clause representation from the root node of a DAG fail to capture this information.
						</aside>
					</div>
				</section>
				
				
				
				<section>
					<div class="box" style="font-size:0.5em;">
						<h4><span class="green emph">NumDistinctVars</span> Explicit Clause Objectives</h4>
						<img class="figure" src="images/NumDistinctVars.png"><br>
						
						<aside class="notes">
							The four best models for this objective are all <span class="emph green">DAGEmbeddingGeom and SimpleEmbedding</span> models.
							Below the visible portion of this plot is <span class="emph green">RandomEmbedding</span> at approximately 25% accuracy, and 
							<span class="emph green">VAEEmbedding, AEEmbedding\_base, and AEEmbedding</span> at approximately 40% accuracy.
							This suggests that for this objective aggregation is again much more important than hierarchical processing.
						</aside>
					</div>
				</section>
				
				
				
				<section>
					<div class="box" style="font-size:0.4em;">
						<h4><span class="green emph">NumEqualities</span> Explicit Clause Objectives</h4>
						<img class="figure" src="images/NumEqualities.png"><br>
						
						<aside class="notes">
							The <span class="emph green">SimpleEmbedding</span> does quite well at this task, probably because the sum of all random symbol vectors
							includes the sum of all "=" symbol vectors. The classifier can then learn to extract this objective via a simple dot product 
							of the <span class="emph green">SimpleEmbedding</span> output with the symbol vector for "=".
							The unrefined autoencoding models fail because information about "=" is always either at depth two or three in the DAG, and that information
							can easily be thrown away by a random initialization.
							Below the visible portion of this plot is the <span class="emph green">RandomEmbedding</span> at approximately 59% accuracy.
						</aside>
					</div>
				</section>
				
				
				
				
				<section>
					<div class="box" style="font-size:0.5em;">
						<h4><span class="green emph">NumNegations</span> Explicit Clause Objectives</h4>
						<img class="figure" src="images/NumNegations.png"><br>
						
						<aside class="notes">
							Although <span class="emph green">NumNegations</span> should be just as easy to learn as <span class="emph green">NumEqualities</span> for <span class="emph green">SimpleEmbedding</span>,
							<span class="emph green">SimpleEmbedding</span> fails to outperform the other models. Perhaps this is because the other models are all easily able to incorporate 
							the necessary information into their embeddings (all "$\sim$" symbols occur as immediate children of the root), and 
							because their additional parameters enable them to eliminate noise from other symbols.
							Below the visible portion of this plot is the <span class="emph green">RandomEmbedding</span> at approximately 25% accuracy.
						</aside>
					</div>
				</section>
				
				
				
				
				<section>
					<div class="box" style="font-size:0.5em;">
						<h4><span class="green emph">MaxDepth</span> Explicit Clause Objectives</h4>
						<img class="figure" src="images/MaxDepth.png"><br>
						
						<aside class="notes">
							Recall that the autoencoding models use the root node of the DAG as the representation of the DAG, as opposed to the max pooling used by the other models.
							In this plot, we see that the <span class="emph green">MaxDepth</span> objective is best learned by the refined autoencoding models, and that
							the unrefined autoencoding models perform worst (on par with random embeddings at approximately 30%).
						</aside>
					</div>
				</section>
				
				

				<section>
					<div>
						<h4> <span class="green emph">Given Clause Selection</span></h4>
						<div class="row" style="gap:1em;">
							<img class="figure" src="images/unrefined.png">
							<img class="figure" src="images/refined.png">
						</div>
					</div>
				</section>
			</section>


			<section>
				<h3>Conclusion and Insights</h3>
				<div class="box small">
					<ul>
						<li class="fragment">Simple models go a long way</li>
						<li class="fragment">
							Problem formulation is important
							<ul>
								<li>Which clauses should the context include?</li>
								<li>Should the context change during training?</li>
								<li>Should (or how should) context clauses become a single context vector?</li>
								<li>How should context affect the given clause selection?</li>
								<li>How should given clauses be labelled?</li>
								<li>What forms of regularization are useful in this domain?</li>
							</ul>
						</li>
						<li class="fragment">The failure of my clause autoencoders perhaps hints that the GNN models I've tried fail to incorporate data from deep in the tree.</li>
					</ul>
				</div>
			</section>

			<section>
				<h3>Future Research</h3>
				<div class="box small">
					<ul>
						<li class="fragment">More nuanced labeling of training data</li>
						<!-- <li class="fragment">Iterated Axiom Selection</li> -->
						<li class="fragment">Improved Tools
							<ul>
								<li class="fragment">Simpler interaction with E</li>
								<li class="fragment">Unified data preprocessing, model training, and evaluation scripts</li>
							</ul>
						</li>
						<li class="fragment">Reinforcement learning</li>
						<li class="fragment">Suggestions?</li>
					</ul>
				</div>
			</section>

			<section>
				<h3>Questions?</h3>
				<h4>Thank you</h4>
			</section>

			<section>

			</section>

		</div>
	</div>

	<script src="dist/reveal.js"></script>
	<script src="plugin/math/math.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			slideNumber: true,
			hash: true,
			math: {
				mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
				config: 'TeX-AMS_HTML-full',
				// pass other options into `MathJax.Hub.Config()`
				TeX: { Macros: { RR: "{\\bf R}" } }
			},
			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
		});
	</script>
</body>

</html>